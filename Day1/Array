An array is a data structure used to store a collection of elements (usually of the same data type) in a contiguous block of memory.
Each element in the array is accessed by its index, starting from 0.

üìå When to Use an Array
Use arrays when:
You need fixed-size storage.
You want fast access to elements using an index.
The data type is the same (e.g., all integers or all strings).


‚úÖ Advantages of Arrays
Fast access: Constant time O(1) access using index.
Memory efficiency: Less overhead compared to dynamic structures.
Cache-friendly: Stored in contiguous memory, improving performance.
‚ùå Disadvantages of Arrays
Fixed size: You must define the size up front.
Costly insertion/deletion: Shifting elements is required.
Same data type only: No mix of types (in strongly typed languages).

üßë‚Äçüíª Example Code in Java 
public class Main {
    public static void main(String[] args) {
        // Creating an array of integers
        int[] numbers = {10, 20, 30, 40, 50};

        // Accessing an element using its index
        System.out.println(numbers[2]);  // Output: 30

        // Modifying an element
        numbers[2] = 35;

        // Looping through the array
        for (int i = 0; i < numbers.length; i++) {
            System.out.println(numbers[i]);
        }
    }
}

üìç Where Arrays are Used
Storing fixed-size collections like days of the week or grades.
Implementing matrices, buffers, queues, and stacks.
In algorithms like sorting, searching, and hashing.

Operations on Arrays with Time Complexity
Arrays are a simple, fixed-size data structure where elements are stored in contiguous memory locations.
Below are the common operations you can perform on arrays along with their time complexities:

1. Accessing an Element
Operation: Accessing an element at a given index in the array.
Time Complexity: O(1) (Constant Time)
Explanation: Since arrays provide direct access to each element using an index (e.g., arr[i]), it takes constant time to access any element in the array.
Example:
int element = arr[2];  // Accessing the element at index 2

2. Inserting an Element
Operation: Inserting an element at a specific index in the array.
Time Complexity: O(n) (Linear Time)
Explanation: Inserting an element at a specific position in an array (not at the end) requires shifting all subsequent elements one position to the right to make room for the new element. 
This results in a time complexity of O(n), where n is the number of elements in the array.
Example:

// Insert at index 2
for (int i = arr.length - 1; i > 2; i--) {
    arr[i] = arr[i - 1];  // Shift elements to the right
}
arr[2] = 100;  // Insert 100 at index 2


3. Deleting an Element
Operation: Deleting an element at a specific index in the array.
Time Complexity: O(n) (Linear Time)
Explanation: Deleting an element requires shifting all the subsequent elements one position to the left. 
This operation involves moving n - 1 elements to fill the gap, making the time complexity O(n).
Example:
// Delete element at index 2
for (int i = 2; i < arr.length - 1; i++) {
    arr[i] = arr[i + 1];  // Shift elements to the left
}

4. Appending an Element
Operation: Appending an element at the end of the array.
Time Complexity: O(1) (Constant Time)
Explanation: Appending an element to the end of an array, if there is space, is a constant time operation since the last index is directly accessible.
Example:
arr[arr.length - 1] = 100;  // Append 100 to the end of the array
Note: In dynamic arrays like ArrayList in Java or List in Python, appending is amortized O(1), but if the array is full and needs to be resized, the resizing operation takes O(n) time, though this is rare.

5. Searching for an Element
Operation: Searching for an element in an array.
Time Complexity: O(n) (Linear Time)
Explanation: In an unsorted array, we need to check each element one by one until we find the target element. This results in O(n) time complexity.
Example:
boolean found = false;
for (int i = 0; i < arr.length; i++) {
    if (arr[i] == target) {
        found = true;
        break;
    }
}
If the array is sorted, you can use binary search (explained below), which reduces the time complexity to O(log n).


6. Binary Search (on Sorted Array)
Operation: Finding an element in a sorted array using binary search.
Time Complexity: O(log n) (Logarithmic Time)
Explanation: In binary search, you repeatedly divide the search space in half.
At each step, you compare the middle element with the target and eliminate half of the remaining elements, reducing the search space logarithmically.
Example:

int left = 0, right = arr.length - 1;
while (left <= right) {
    int mid = left + (right - left) / 2;
    if (arr[mid] == target) {
        return mid;  // Found the element
    }
    if (arr[mid] < target) {
        left = mid + 1;  // Search in the right half
    } else {
        right = mid - 1;  // Search in the left half
    }
}

7. Reversing the Array
Operation: Reversing the order of elements in the array.
Time Complexity: O(n) (Linear Time)
Explanation: To reverse an array, you need to swap elements from both ends. This operation involves iterating through the array once, which gives O(n) time complexity.
Example:
for (int i = 0; i < arr.length / 2; i++) {
    int temp = arr[i];
    arr[i] = arr[arr.length - 1 - i];
    arr[arr.length - 1 - i] = temp;
}

8. Copying an Array
Operation: Copying all elements from one array to another.
Time Complexity: O(n) (Linear Time)
Explanation: To copy an array, each element must be accessed and copied to the new array, so the time complexity is proportional to the size of the array, i.e., O(n).
Example:
int[] newArr = new int[arr.length];
for (int i = 0; i < arr.length; i++) {
    newArr[i] = arr[i];  // Copy each element
}

9. Sorting the Array
Operation: Sorting an array (e.g., using quick sort, merge sort, etc.)
Time Complexity: O(n log n) (for efficient sorting algorithms like quicksort/mergesort)
Explanation: Sorting an array typically uses algorithms like quick sort or merge sort, which have a time complexity of O(n log n) in the average case. 
However, some simpler algorithms like bubble sort or insertion sort have a worst-case time complexity of O(n^2).
Example (Quick Sort):
public void quickSort(int[] arr, int low, int high) {
    if (low < high) {
        int pi = partition(arr, low, high);
        quickSort(arr, low, pi - 1);
        quickSort(arr, pi + 1, high);
    }
}


Ques : Why do most programming languages (like Java, C, Python) use 0-based indexing for arrays instead of 1-based indexing, and what are the advantages of using 0-based indexing?

Answer:
1. Historical and Low-Level Memory Access:
The choice to start array indexing at 0 comes from low-level programming languages like C and assembly. 
In these languages, arrays are implemented as pointers to contiguous memory blocks.
When you access an array element, its memory address is calculated by adding an offset to the base address (the starting memory location of the array). 
With 0-based indexing, the first element is directly at the base address. If we used 1-based indexing, 
we would have to subtract 1 from the index during the address calculation, which introduces unnecessary complexity and reduces efficiency.

2. Efficient Pointer Arithmetic:
In 0-based indexing, calculating the address of an element is straightforward:
Address of arr[i] = Base address + (i * size of element)
This formula works directly with 0 as the starting index.
For 1-based indexing, the formula becomes:
Address of arr[i] = Base address + ((i - 1) * size of element)
The adjustment i - 1 adds an extra step in the calculation, making pointer arithmetic less efficient.


3. Consistency in Algorithm Design:
Many algorithms (especially in graph theory, sorting algorithms, and binary trees) are naturally designed to work with 0-based indexing. 
For example, in a binary heap or binary tree, the children of an element can be calculated using simple formulas:
Left child: 2 * index + 1
Right child: 2 * index + 2
These calculations are much cleaner and simpler when starting with 0.

4. Influence of Early Languages (C and Assembly):
C and assembly (which influenced many modern programming languages) started with 0-based indexing, and that tradition carried forward into other languages like Java, Python, JavaScript, etc.
These languages are used for system-level programming where performance, memory access, and efficiency are critical.

5. Memory Layout and Cache Efficiency:
Arrays are stored as contiguous blocks in memory, and starting at index 0 allows sequential memory access to be more efficient. 
The memory layout works better with 0-based indexing because the first element is at the base address without additional offset adjustments.
Advantages of 0-based Indexing:
Efficiency in memory address calculation (simpler and faster).
Consistent with pointer arithmetic, which is easier to implement.
Cleaner algorithm design for data structures like heaps, binary trees, etc.
Less computational overhead compared to 1-based indexing, particularly in low-level languages.
Standardization: Most modern languages have inherited the 0-based indexing convention from C and other early languages.
